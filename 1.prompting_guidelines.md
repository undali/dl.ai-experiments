# Prompting Guidelines for Language Models

## Overview

This guide covers principles and tactics for effective prompt engineering with language models like ChatGPT (GPT-3.5 Turbo).

---

## Two Key Principles

### Principle 1: Write Clear and Specific Instructions
Express what you want the model to do with instructions that are as clear and specific as possible. This guides the model toward desired output and reduces irrelevant or incorrect responses.

**Note:** Clear â‰  Short. Longer prompts often provide more clarity and context, leading to more detailed and relevant outputs.

### Principle 2: Give the Model Time to Think
If a model makes reasoning errors by rushing to conclusions, reframe the query to request a chain of reasoning before the final answer. Complex tasks need more computational effort.

---

## Principle 1: Tactics for Clear Instructions

### Tactic 1: Use Delimiters
Use delimiters to clearly indicate distinct parts of the input.

**Examples of delimiters:**
- Triple backticks: ```
- Quotes
- XML tags
- Section titles

**Example prompt:**
```
Summarize the text delimited by triple backticks into a single sentence.
```text here```
```

**Benefits:**
- Makes it clear which text to process
- Helps avoid prompt injections (when users try to override instructions)

### Tactic 2: Ask for Structured Output
Request structured formats like HTML or JSON to make parsing easier.

**Example prompt:**
```
Generate a list of three made-up book titles along with their authors and genres. 
Provide them in JSON format with the following keys: book_id, title, author, genre.
```

**Benefit:** Output can be easily parsed into dictionaries or lists in code.

### Tactic 3: Check Whether Conditions Are Satisfied
Have the model verify assumptions before proceeding with the task.

**Example prompt:**
```
You'll be provided with text delimited by triple quotes. 
If it contains a sequence of instructions, rewrite those instructions in the following format:
Step 1 - ...
Step 2 - ...

If the text does not contain a sequence of instructions, then simply write "No steps provided."
```

**Benefit:** Handles edge cases and avoids unexpected errors.

### Tactic 4: Few-Shot Prompting
Provide examples of successful task execution before asking the model to perform the actual task.

**Example:**
```
Your task is to answer in a consistent style.

<child>: Teach me about patience.
<grandparent>: The river that carves the deepest valley flows from a modest spring...

<child>: Teach me about resilience.
```

**Benefit:** Model responds in a consistent tone matching the examples.

---

## Principle 2: Tactics for Giving Time to Think

### Tactic 1: Specify Steps Required to Complete a Task
Break down complex tasks into explicit steps.

**Example prompt:**
```
Perform the following actions:
1. Summarize the following text delimited by triple backticks with one sentence
2. Translate the summary into French
3. List each name in the French summary
4. Output a JSON object with keys: french_summary, num_names

Separate your answers with line breaks.
```

**Advanced format (more standardized):**
```
Use the following format:
Text: <text to summarize>
Summary: <summary>
Translation: <translation>
Names: <names>
Output JSON: <json>
```

**Benefit:** Standardized format makes output easier to parse with code.

### Tactic 2: Instruct the Model to Work Out Its Own Solution
Have the model reason through problems before evaluating answers.

**Example scenario:** Checking if a student's math solution is correct.

**Better prompt:**
```
Your task is to determine if the student's solution is correct or not.
To solve the problem, do the following:
- First, work out your own solution to the problem
- Then compare your solution to the student's solution
- Evaluate if the student's solution is correct or not
- Don't decide if the student's solution is correct until you have done the problem yourself

Use the following format:
Question: <question>
Student's solution: <student's solution>
Actual solution: <your solution>
Is the student's solution correct: <yes/no>
Student grade: <correct/incorrect>
```

**Benefit:** Breaking down tasks and giving the model time to think leads to more accurate responses.

---

## Model Limitations: Hallucinations

### What Are Hallucinations?
The model may try to answer questions about obscure topics and fabricate plausible-sounding but incorrect information. This happens because:
- The model hasn't perfectly memorized all training information
- It doesn't know the boundaries of its knowledge well

### Example
**Prompt:** "Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie"

The model will generate a realistic-sounding description of a fictitious product.

### How to Reduce Hallucinations
Ask the model to:
1. First find relevant quotes from the provided text
2. Then use those quotes to answer questions
3. Trace answers back to source documents

This helps ground responses in actual information rather than fabrications.

---

## Setup Notes

### Using OpenAI API
```python
import openai

# Set API key
openai.api_key = "your-api-key-here"

# Helper function
def get_completion(prompt, model="gpt-3.5-turbo"):
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0
    )
    return response.choices[0].message["content"]
```

---

## Key Takeaways

1. **Be clear and specific** - Don't confuse this with being short
2. **Use delimiters** - Separate different parts of your prompt clearly
3. **Request structured output** - JSON, HTML, etc. for easier parsing
4. **Verify assumptions** - Have the model check conditions before proceeding
5. **Provide examples** - Few-shot prompting helps maintain consistency
6. **Specify steps** - Break complex tasks into explicit steps
7. **Allow reasoning time** - Have the model work through problems before answering
8. **Watch for hallucinations** - Always verify important information
9. **Ground in sources** - Ask for quotes/references to reduce fabrication

---

## Next Steps

Move on to iterative prompt development process to refine and improve your prompts based on results.
